{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Compression With Low Rank Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**IMPORTANT:** The responses to the questions must be in your written submission, not in the notebook. You don't need to submit the notebook. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "The motivation speaks for itself here. We have all seen just how fast our image folders blow up in size as time goes on. It is advantageous to be able to represent the same image with less data, which is what compression is. We will explore how to do this by means of low rank approximation via the SVD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Decomposing Images Into Rank 1 Components\n",
    "\n",
    "In this part, we will investigate the underlying structure of images by decomposing them into sums of rank 1 matrices. Remember that the outer product of two vectors $\\vec{a} \\vec{b}^T$ gives a rank 1 matrix. \n",
    "It has rank 1 because the columns are only multiples of $\\vec{a}$, and the rows are only multiples of $\\vec{b}^T$. \n",
    "\n",
    "We will first investigate an image with a 4x4 checkered pattern. We will assume that black squares correspond to -1 and white squares correspond to 1. In matrix form, this will be\n",
    "$$L = \\begin{bmatrix}\n",
    "\t\t\t\t1 & -1 & 1 & -1 \\\\\n",
    "\t\t\t\t-1 & 1 & -1 & 1 \\\\\n",
    "\t\t\t\t1 & -1 & 1 & -1 \\\\\n",
    "\t\t\t\t-1 & 1 & -1 & 1 \\\\\n",
    "\\end{bmatrix} $$\n",
    "We can also display this as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "check = np.array([\n",
    "    [1, -1, 1, -1],\n",
    "    [-1, 1, -1, 1],\n",
    "    [1, -1, 1, -1],\n",
    "    [-1, 1, -1, 1],\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(check, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">**Question (a):** Express $L$ as a linear combination of outer products. </span>\n",
    "\n",
    "_Hint:  In order to determine how many rank 1 matrices you need to combine to represent the matrix, first find the rank of the matrix you are trying to represent._\n",
    "\n",
    "To check your answer we have written a helper function that calculates the outer products and displays them as images. You can type in your outer product vectors below and check if the sum of them looks the same as the checkerboard. Remember to write your answer in the writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check your answer. Not necessary to understand\n",
    "def plot_outer_products(cols, rows, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Plots outer products generated from the given cols and rows.\n",
    "    First row of images contains each individual outer product.\n",
    "    Second row contains the sum of all the outer products.\n",
    "    \n",
    "    Args:\n",
    "        cols: List of 1D Numpy arrays\n",
    "        rows: List of 1D Numpy arrays. Must have same dimension as cols\n",
    "        cmap: matplotlib.Colormap for mapping number values to colors. Default is grayscale\n",
    "    Returns:\n",
    "        sum of all outer product matrices\n",
    "    \"\"\"\n",
    "    assert len(cols) == len(rows), \"Number of column and rows should be the same\"\n",
    "\n",
    "    num_mat = len(cols)\n",
    "    mats = [np.outer(c, r) for (c, r) in zip(cols, rows)]\n",
    "    total = sum(mats)\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_mat, figsize=(5*num_mat, 5))\n",
    "    for i in range(num_mat):\n",
    "        if num_mat == 1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[i]\n",
    "        ax.imshow(mats[i], cmap=cmap)\n",
    "        ax.set_title(f\"Rank 1 matrix of outer product {i}\")\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(total, cmap=cmap)\n",
    "    plt.title(f\"Sum of {num_mat} outer products\")\n",
    "    plt.show()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the column vectors of your outer products in the `cols` array \n",
    "# and the row vectors of your outer products in the `rows` array\n",
    "\n",
    "# Potential solution, there are infinite solutions\n",
    "cols = [np.array([0, 0, 0, 0])]\n",
    "rows = [np.array([0, 0, 0, 0])]\n",
    "\n",
    "total = plot_outer_products(cols, rows, 'gray')\n",
    "print(\"Sum of outer products:\\n\", total)\n",
    "print(\"Arrays equal:\", np.array_equal(total, check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we investigate an image with a 4x4 checkered pattern. Here we will assume that black squares correspond to 0 and white squares correspond to 1. In matrix form, this will be\n",
    "$$C = \\begin{bmatrix}\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1\n",
    "\\end{bmatrix} $$\n",
    "We can also display this as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check = np.array([\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1],\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1],\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(check, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">**Question (b):** Express $C$ as a linear combination of outer products. </span>\n",
    "\n",
    "_Hint:  In order to determine how many rank 1 matrices you need to combine to represent the matrix, first find the rank of the matrix you are trying to represent._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the column vectors of your outer products in the `cols` array \n",
    "# and the row vectors of your outer products in the `rows` array\n",
    "\n",
    "# Potential solution, there are infinite solutions\n",
    "cols = [np.array([0, 0, 0, 0])]\n",
    "rows = [np.array([0, 0, 0, 0])]\n",
    "\n",
    "total = plot_outer_products(cols, rows, 'gray')\n",
    "print(\"Sum of outer products:\\n\", total)\n",
    "print(\"Arrays equal:\", np.array_equal(total, check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a new image of the Swiss flag. In matrix form it can be represented as\n",
    "$$S = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$$\n",
    "where 0 corresponds to red and 1 corresponds to white.\n",
    "Displaying the matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "])\n",
    "\n",
    "swiss_cmap = matplotlib.colors.ListedColormap([\"red\", \"white\"])\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(swiss, cmap=swiss_cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">**Question (c):** Express $S$ as a linear combination of outer products. </span>\n",
    "\n",
    "_Hint:  In order to determine how many rank 1 matrices you need to combine to represent the matrix, first find the rank of the matrix you are trying to represent._\n",
    "\n",
    "You can use the same helper function as before to check your answer, by typing in your outer product vectors below. Remember to write your answer in the writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the column vectors of your outer products in the `cols` array \n",
    "# and the row vectors of your outer products in the `rows` array\n",
    "\n",
    "# Potential solution\n",
    "cols = [np.array([0, 0, 0, 0, 0])]\n",
    "rows = [np.array([0, 0, 0, 0, 0])]\n",
    "\n",
    "total = plot_outer_products(cols, rows, swiss_cmap)\n",
    "print(\"Sum of outer products:\\n\", total)\n",
    "print(\"Arrays equal:\", np.array_equal(total, swiss))\n",
    "\n",
    "# Alternative solution, there are infinite solutions\n",
    "cols = [np.array([0, 1, 1, 1, 0]), np.array([0, 1, 0, 1, 0])]\n",
    "rows = [np.array([0, 1, 1, 1, 0]), np.array([0, -1, 0, -1 , 0])]\n",
    "\n",
    "total = plot_outer_products(cols, rows, swiss_cmap)\n",
    "print(\"Sum of outer products:\\n\", total)\n",
    "print(\"Arrays equal:\", np.array_equal(total, swiss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Low Rank Approximation of Images\n",
    "\n",
    "We now generalize what we did in the previous part by finding the rank 1 decomposition of any matrix using the SVD. Then we will choose which of these rank 1 matrices contribute the most to the matrix and keep those, throwing the remaining rank 1 matrices away.\n",
    "\n",
    "### Low Rank Approximation - The Math\n",
    "Suppose we have a matrix $A \\in \\mathbb{R}^{m \\times n}$ that represents a block of data (could be an image, where each entry of the matrix corresponds to a pixel value). To store A, we typicall require $mn$ real numbers. \n",
    "\n",
    "The SVD of $A$ is given below. We know that if we have $r$ non-zero singular values, this corresponds to a rank $r$ matrix. Therefore, $r$ columns of $U$ and $V$ respectively will matter for computing $A$. But if some of the singular values are very small, we might be able to approximately preserve $A$ while reducing the storage required for $A$. This is called the 'Low Rank Approximation' since you want to go from $r$ singular values to a smaller number $k$ singular values, reducing the rank of $A$ to $k$. \n",
    "\n",
    "$$\n",
    "A = U  \n",
    "\\begin{bmatrix} \n",
    "    \\sigma_1 & 0 & \\cdots & 0 & 0\\\\ \n",
    "    0 & \\ddots & 0 & 0 & 0\\\\\n",
    "    \\vdots & 0 & \\sigma_r & 0 & 0\\\\ \n",
    "    \\vdots & \\vdots & 0 & \\vdots & \\vdots\\\\ \n",
    "    0 & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
    "    0 & 0 & \\cdots & 0 & 0\\\\\n",
    "    & & \\vdots \\\\\n",
    "    0  & 0 & \\cdots & 0 & 0\n",
    "\\end{bmatrix} V^\\top = U \\begin{bmatrix}\n",
    "\\Sigma_r & 0_{r \\times (n - r)} \\\\\n",
    "0_{(m-r) \\times r} & 0_{(m-r) \\times (n-r)}\n",
    "\\end{bmatrix} V^\\top\n",
    "\\\\\n",
    "A = \\sum_{i=1}^r \\sigma_i \\vec{u_i}\\vec{v_i}^\\top\n",
    "$$\n",
    "\n",
    "We would like to  find the best rank $\\le k$ approximation to $A$. Let us call this approximation $A_k$. The more mathematical formulation of $A_k$ is \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "A_k = \\underset{rank(\\widehat{A}_k) \\le k}{\\operatorname{argmin}} ||A - \\widehat{A_k}||_F.\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "If we let $A = U \\Sigma V^\\top$, then the best rank $\\le k$ approximation to A is \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "A_k = U  \n",
    "\\begin{bmatrix} \n",
    "    \\sigma_1 & 0 & \\cdots & 0 & 0\\\\ \n",
    "    0 & \\ddots & 0 & 0 & 0\\\\\n",
    "    \\vdots & 0 & \\sigma_k & \\vdots & \\vdots \\\\ \n",
    "    0 & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
    "    0 & 0 & \\cdots & 0 & 0\\\\\n",
    "    & & \\vdots \\\\\n",
    "    0  & 0 & \\cdots & 0 & 0\n",
    "\\end{bmatrix} V^\\top = U \\begin{bmatrix}\n",
    "\\Sigma_k & 0_{k \\times (n - k)} \\\\\n",
    "0_{(m-k) \\times k} & 0_{(m-k) \\times (n-k)}\n",
    "\\end{bmatrix} V^\\top\n",
    "\\\\\n",
    "A_k = \\sum_{i=1}^k \\sigma_i \\vec{u_i}\\vec{v_i}^\\top.\n",
    "$$\n",
    "\n",
    "In other words, the best rank $\\le k$ approximation to $A$ is the SVD of $A$, but we only keep the first $k$ singular values in the $\\Sigma$ matrix and zero out all other entries. In outer product form, this is the same as making the upper index $k$ instead of $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship to Images and Compression\n",
    "We know that $A$ has $m$ rows and $n$ columns. This means that if we were store the matrix $A$ on a machine, we would have to store $mn$ elements. However, using the outer product form of $A_k$, we can see that there will be a singular value, a length $m$ vector and a length $n$ vector in the $i^{th}$ term of the summation. So, that puts us at $k(m + n + 1)$ total elements to store the rank $k$ approximation to $A$.\n",
    "\n",
    "So, if we approximate $A$ with the best rank $\\le k$ matrix $A_k$, we only need $k (m + n + 1)$ elements instead $mn$ elements for the full matrix at the cost of losing information. \n",
    "\n",
    "In this notebook, the matrix $A$ will represent an image with $m$ rows and $n$ columns. Each entry of $A$ will be a value between 0 and 255, representing the brightness of a grayscale pixel. \n",
    "\n",
    "The idea with image compression is to remove just enough information so that our eyes cannot notice the difference. So, if we approximate our matrix (image) $A$ with $A_k$, we will definitely lose quality in the resulting image. **The goal is to make $k$ as small as possible without losing too much quality.**\n",
    "\n",
    "Let us start by importang a grayscale image of a kitten and representing the image as a Numpy matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_image_matrix(A, title='', fig_size=(7,7)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.title(title)\n",
    "    plt.imshow(A, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kitten\n",
    "A_kitten = plt.imread(\"images/kitten.PNG\") \n",
    "\n",
    "# Plot the kitten image\n",
    "plot_image_matrix(A_kitten)\n",
    "\n",
    "# Display the dimensions of A \n",
    "print(\"Shape of A_kitten:\", A_kitten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dimension of $A$ is $901$ by $1200$. That is, we have $901$ columns and $1200$ rows in our image. So, there are $901 * 1200 = 1081200$ pixels. This is roughly 1 megapixel, a lot of information! So, let us proceed by compressing the image via low rank approximation.\n",
    "\n",
    "We will need to construct a function that takes in a matrix $A$ and value $k$ and return the best rank $\\le k$ approximation of A. We will step through the process slowly, and the consolidate everything into this big function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the Low Rank Approximation\n",
    "So, as we saw in the math section, we know that we need the SVD of A. We start by computing the compact SVD of A. We are not making use of the full SVD at all here, so it is more efficient to avoid the extra Gram-Schmidt step and use the compact SVD. In Numpy, this means we set the `full_matrices` argument to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute U, Vt, and singular values of A\n",
    "U, sing_vals, Vt = np.linalg.svd(A_kitten, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let us pick the top $k=200$ singular values. We will talk more about how to choose better values of $k$ soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick top k singular values\n",
    "k = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start our approximation. Let us first repaste the mathematical expression for the rank $k$ approximation of $A$:\n",
    "\n",
    "$$\n",
    "A_k = \\sum_{i=1}^{k}\\sigma_i \\vec{u}_i \\vec{v}_i^\\top\n",
    "$$\n",
    "\n",
    "So, we can directly code this up and plot our resulting image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticing that we need the first $k$ columns of $U$, the first $k$ rows of $V$, and the first $k$ singular values, we can store these into variables for later: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First k columns of U\n",
    "U_k = U[:,:k]\n",
    "# First k singular values\n",
    "sing_vals_k = sing_vals[:k]\n",
    "# First k rows of V^T\n",
    "Vt_k = Vt[:k,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive implimentation of the rank k approximation\n",
    "A_k_naive = np.zeros_like(A_kitten)\n",
    "for i in range(k):\n",
    "    A_k_naive += sing_vals_k[i] * np.outer(U_k[:,i], Vt_k[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation works just fine, however it does not exploit what Numpy has to offer. So instead of using a for loop, we will convert the summation back to matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better implementation of the rank k approximation\n",
    "A_k_better = np.dot(U_k * sing_vals_k, Vt_k)\n",
    "\n",
    "# Make sure both implementations agree\n",
    "assert np.allclose(A_k_better, A_k_naive, atol=1e-5)\n",
    "\n",
    "# Use the better one from now on\n",
    "A_k = A_k_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this any better? It turns out that in general we should avoid writing our own for loops for speed purposes. This is because the Numpy library has optimized algorithms for matrix multiplication which also take into account your specific hardware, such as your memory cache. This is not so important for the main idea, but now you know why we would want to use Numpy operations rather than a for loop.\n",
    "\n",
    "We now combine the work we just did into 1 function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_k_approx(A, k, return_components=False):\n",
    "    # Compute U, Vt, and singular values of A\n",
    "    U_r, sing_vals, Vt_r = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # First k columns of U\n",
    "    U_k = U_r[:,:k]\n",
    "    # First k singular values\n",
    "    sing_vals_k = sing_vals[:k]\n",
    "    # First k rows of V\n",
    "    Vk = Vt_r[:k,:]\n",
    "    \n",
    "    # Better implimentation of the rank k approximation\n",
    "    A_k = np.dot(U_k * sing_vals_k, Vk)\n",
    "    \n",
    "    if return_components:\n",
    "        # Return the approximation and the first k SVD components\n",
    "        return A_k, U_k, sing_vals_k, Vk\n",
    "    else:\n",
    "        # Just return the rank k approximation of A\n",
    "        return A_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">**Question (d):** Since we are now only using the top $k$ singular values, in terms of $k, m,$ and $n$, how many real numbers do we require to describe the rank $k$ approximation of $A$?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kitten Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start our analysis, we will write a function called `mem_savings` that will take in a matrix $A$ and value $k$ and tell us how much memory we saved using the rank $k$ approximation. Remember that we only need $k (m + n + 1)$ elements for the rank $k$ approximation and $m n$ elements for the full matrix. We will compute $100 \\frac{k(m+n + 1)}{mn}$, or the percentage of the compressed data to the full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_savings(A, k):\n",
    "    # Get m and n\n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Rank k number of entries\n",
    "    approx_ent = k * (m + n + 1)\n",
    "    \n",
    "    # Full matrix number of entries\n",
    "    full_ent = m * n\n",
    "    \n",
    "    # Return percentage\n",
    "    return 100 * approx_ent / full_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to use our functions! Let us start with $k = 200$ for the kitten and find the rank $k$ approximation for `A_kitten`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rank k approximation of the kitten\n",
    "A_kitten_k, Uk, sigma_k, Vk  = rank_k_approx(A_kitten, k=200, return_components=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing our memory savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnt_of_full = mem_savings(A_kitten, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, lets compare the full rank and low rank images. We will also display our percent memory savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both pictures\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot low rank\n",
    "ax1.set_title(f'Kitten Rank {k} Compression')\n",
    "ax1.imshow(A_kitten_k, cmap='gray')\n",
    "\n",
    "# Plot full rank\n",
    "ax2.set_title(f'Uncompressed Kitten')\n",
    "ax2.imshow(A_kitten, cmap='gray')\n",
    "\n",
    "# Memory Saved\n",
    "print(f'Using {pcnt_of_full:.2f}% of full image for a rank {k} approximation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">**Question (e):** Can you easily visually detect any difference between the two images even though we are using only about 40% of the data? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we are able to preserve image quality with only about 40% of the data is because the matrix $A$ must have that its top $k$ singular values are much larger than the remaining singular values. We will this visually in the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us talk about how to choose better values of k. Remember, we want a rank $\\le k$ approximation to $A$. How do we choose $k$? It may help to recall the outer product expansion of A:\n",
    "\n",
    "$$\n",
    "A = \\sum_{i=1}^{min(m,n)} \\sigma_i \\vec{u_i} \\vec{v_i}^\\top\n",
    "$$\n",
    "\n",
    "We can see that looking at the distribution of the singular values will help us figure out where most of the data is captured. This is best described by a plot. We will provide both a standard and semilog plot to get the full picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute U, Vt, and singular values of A_kitten\n",
    "U_r, sing_vals, Vt_r = np.linalg.svd(A_kitten, full_matrices=False)\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot singular values - standard\n",
    "ax1.plot(sing_vals)\n",
    "ax1.set_xlabel(\"$i$\")\n",
    "ax1.set_ylabel(\"$\\sigma_i$\")\n",
    "ax1.set_title(\"$i$ vs $\\sigma_i$\")\n",
    "\n",
    "# Plot singular values - log\n",
    "ax2.semilogy(sing_vals)\n",
    "ax2.set_xlabel(\"$i$\")\n",
    "ax2.set_ylabel(\"log scale of $\\sigma_i$\")\n",
    "ax2.set_title(\"log scale $i$ vs $\\sigma_i$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Question (f):** Given the plots above, why did the image look so good even with only about 40% of the data? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets figure out how varying $k$ will change the image. We have attatched a slider plot below. Try out different values of $k$ and answer the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Turn on interactive plotting\n",
    "%matplotlib notebook\n",
    "# Setup figure \n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "im = ax.imshow(A_kitten, cmap='gray')\n",
    "\n",
    "# Determine slider min and max\n",
    "k_min = 0\n",
    "k_max = 200\n",
    "\n",
    "# Get initial matrices\n",
    "A_kitten_k, U_k, sigma_k, V_k = rank_k_approx(A_kitten, k_max, return_components=True)\n",
    "\n",
    "# On each update we compute the low rank approximation\n",
    "def update(k=1.0):\n",
    "    # Compute low rank approximation\n",
    "    # We aren't using the low rank function here to avoid recomputing the SVD every time, which takes a while\n",
    "    A_k = np.dot((sigma_k * U_k)[:,:k],  V_k[:k,:])\n",
    "    # Compute size\n",
    "    ax.imshow(A_k, cmap='gray')\n",
    "    ax.set_title(f'{mem_savings(A_kitten, k):.2f}% of total data needed for rank {k}')\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update, k=widgets.IntSlider(min=k_min, max=k_max, step=1, layout=Layout(width='80%')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Question (f):** What is the lowest acceptable value of $k$ that you can go without losing too much image quality? What were the memory savings? </span>\n",
    "\n",
    "Just use your best judgment since this depends on eyesight, screen resolution, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Simpler Image Analysis - US Flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will analyze the US flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import the flag\n",
    "A_flag = plt.imread(\"images/USA.PNG\") \n",
    "print(\"Image shape:\", A_flag.shape)\n",
    "\n",
    "# Show image\n",
    "plot_image_matrix(A_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will start off by computing the SVD and plotting out the singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute singular values of A_block\n",
    "_, sing_vals, _ = np.linalg.svd(A_flag, full_matrices=False)\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot singular values - standard\n",
    "ax1.plot(sing_vals)\n",
    "ax1.set_xlabel(\"$i$\")\n",
    "ax1.set_ylabel(\"$\\sigma_i$\")\n",
    "ax1.set_title(\"$i$ vs $\\sigma_i$\")\n",
    "\n",
    "# Plot singular values - log\n",
    "ax2.semilogy(sing_vals + 1e-1)\n",
    "ax2.set_xlabel(\"$i$\")\n",
    "ax2.set_ylabel(\"$\\sigma_i$\")\n",
    "ax2.set_title(\"$i$ vs $\\sigma_i$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Question (g):** What do you notice about the singular values here in comparison to the kitten's singular values? What does this mean for our low rank compression?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify your answers above, play around with the slider for the block image and then answer the questions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on interactive plotting\n",
    "%matplotlib notebook\n",
    "# Setup figure \n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "im = ax.imshow(A_flag, cmap='gray')\n",
    "\n",
    "# Determine slider min and max\n",
    "k_min = 0\n",
    "k_max = 100\n",
    "\n",
    "# Get initial matrices\n",
    "A_block_k, U_k, sigma_k, V_k = rank_k_approx(A_flag, k_max, return_components=True)\n",
    "\n",
    "# On each update we compute the low rank approximation\n",
    "def update(k=1.0):\n",
    "    # Compute low rank approximation\n",
    "    # We are not using the low rank function here to avoid recomputing the svd everytime.\n",
    "    A_k = np.dot((U_k * sigma_k)[:,:k],  V_k[:k,:])\n",
    "    # Compute size\n",
    "    ax.imshow(A_k, cmap='gray')\n",
    "    ax.set_title(f'{mem_savings(A_flag, k):.2f}% of total data needed for rank {k}')\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update, k=widgets.IntSlider(min=k_min, max=k_max, step=1, layout=Layout(width='80%')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Question (h):** What is the lowest acceptable value of $k$ here? What is the memory saving? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice a specific structure if you set $k=1$? No need to formally answer this, just for your curiosity.\n",
    "\n",
    "The structure we see for $k=1$ is exactly scaled versions of a vector in each column of the image. This helps us see why the flag was much more compressable than the kitten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
